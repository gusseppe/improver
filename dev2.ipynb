{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich import print\n",
    "import inspect\n",
    "from typing import List, Dict, Any\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "# Usage\n",
    "\n",
    "\n",
    "load_dotenv(\"env\")\n",
    "\n",
    "class NodeFunctionUpdater:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def update(self, node_name: str, original_function: str, score: float, feedback: str) -> str:\n",
    "        print(f\"Updating node: {node_name}\")\n",
    "        print(f\"Original function:\\n{original_function}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        prompt_template = f\"\"\"\n",
    "        You are an expert in optimizing Python functions. I have a function that needs to be improved based on the \n",
    "        performance score and feedback provided. Please rewrite the function following the template provided below.\n",
    "        Original function:\n",
    "        ```\n",
    "        {original_function}\n",
    "        ```\n",
    "        Performance score: {score}\n",
    "        Feedback: {feedback}\n",
    "        \n",
    "        The improved function should follow this template:\n",
    "\n",
    "        def {node_name}(state: AgentState) -> AgentState:\n",
    "            def updated_node() -> ChatPromptTemplate:\n",
    "                examples = [\n",
    "                    {{\n",
    "                        \"input\": textwrap.dedent('''\n",
    "                        [HERE GOES THE INPUT 1]\n",
    "                        '''),\n",
    "                        \"output\": textwrap.dedent('''\n",
    "                        [HERE GOES THE OUTPUT 1]            \n",
    "                        ''')\n",
    "                    }},\n",
    "                    {{\n",
    "                        \"input\": textwrap.dedent('''\n",
    "                        [HERE GOES THE INPUT 2]\n",
    "                        '''),\n",
    "                        \"output\": textwrap.dedent('''\n",
    "                        [HERE GOES THE OUTPUT 2] \n",
    "                        ''')\n",
    "                    }}\n",
    "                ]\n",
    "                \n",
    "                example_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"human\", \"{{input}}\"),\n",
    "                        (\"ai\", \"{{output}}\"),\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "                    example_prompt=example_prompt,\n",
    "                    examples=examples,\n",
    "                )\n",
    "                system_prompt = textwrap.dedent('''\n",
    "                [HERE GOES THE SYSTEM PROMPT]\n",
    "                ''')\n",
    "                final_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", system_prompt),\n",
    "                        few_shot_prompt,\n",
    "                        (\"human\", \"{{input}}\"),\n",
    "                    ]\n",
    "                )\n",
    "                return final_prompt\n",
    "\n",
    "            prompt = updated_node()\n",
    "            chain = prompt | self.llm\n",
    "            result = chain.invoke({{\"input\": state['requirements']}})\n",
    "            return {{\"email\": result.content}}\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"original_function\": original_function,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "        print(\"LLM response content:\")\n",
    "        print(response.content)\n",
    "        return response.content\n",
    "\n",
    "class GraphAgentOptimizer:\n",
    "    def __init__(self, original_graph: StateGraph, ground_truth: List[Dict[str, Any]], llm):\n",
    "        self.original_graph = original_graph\n",
    "        self.ground_truth = ground_truth\n",
    "        self.llm = llm\n",
    "        self.node_functions = {}\n",
    "        self.node_performances = {}\n",
    "        self.node_updater = NodeFunctionUpdater(llm)\n",
    "\n",
    "    def load_graph(self):\n",
    "        print(\"Loading graph...\")\n",
    "        for node_name, node_func in self.original_graph.nodes.items():\n",
    "            self.node_functions[node_name] = self.extract_function_source(node_func)\n",
    "            self.node_performances[node_name] = {'score': 0, 'feedback': ''}\n",
    "        print(f\"Loaded {len(self.node_functions)} nodes.\")\n",
    "\n",
    "    def extract_function_source(self, func):\n",
    "        if hasattr(func, 'func'):  # For RunnableCallable objects\n",
    "            return inspect.getsource(func.func)\n",
    "        elif callable(func):\n",
    "            return inspect.getsource(func)\n",
    "        else:\n",
    "            return str(func)  # Fallback for other types\n",
    "\n",
    "    def optimize(self, num_iterations: int):\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "            for i, example in enumerate(self.ground_truth):\n",
    "                print(f\"  Processing example {i + 1}\")\n",
    "                try:\n",
    "                    output = self.forward_pass(example['input'])\n",
    "                    self.evaluate_and_update(output, example['output'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i + 1}: {str(e)}\")\n",
    "            self.update_node_functions()\n",
    "\n",
    "    def forward_pass(self, input_data: str) -> Dict[str, Any]:\n",
    "        print(\"Performing forward pass...\")\n",
    "        state = {\"requirements\": input_data}\n",
    "        for node_name in self.original_graph.nodes:\n",
    "            print(f\"  Executing node: {node_name}\")\n",
    "            node_func = self.get_node_function(node_name)\n",
    "            state.update(node_func(state))\n",
    "        print(\"Forward pass completed.\")\n",
    "        return state\n",
    "\n",
    "    def evaluate_and_update(self, generated_output: Dict[str, Any], ground_truth_output: str):\n",
    "        print(\"Evaluating output...\")\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Compare the generated output with the ground truth output.\n",
    "        Provide a score from 0 to 1 for each node in the graph,\n",
    "        where 1 is perfect and 0 is completely wrong.\n",
    "        Also provide a brief explanation for each score.\n",
    "\n",
    "        Generated output:\n",
    "        {generated_output}\n",
    "\n",
    "        Ground truth output:\n",
    "        {ground_truth_output}\n",
    "\n",
    "        Response format:\n",
    "        [node_name] score: [score]\n",
    "        [node_name] feedback: [feedback]\n",
    "        (Repeat for each node)\n",
    "        \"\"\")\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"generated_output\": str(generated_output),\n",
    "            \"ground_truth_output\": ground_truth_output\n",
    "        })\n",
    "\n",
    "        print(\"Raw LLM response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        lines = response.content.split('\\n')\n",
    "        for node_name in self.node_functions.keys():\n",
    "            score_line = next((line for line in lines if f\"{node_name} score:\" in line), None)\n",
    "            feedback_line = next((line for line in lines if f\"{node_name} feedback:\" in line), None)\n",
    "            \n",
    "            if score_line and feedback_line:\n",
    "                try:\n",
    "                    score = float(score_line.split(':')[1].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse score for {node_name}. Using default score of 0.5\")\n",
    "                    score = 0.5\n",
    "                \n",
    "                feedback = feedback_line.split(':', 1)[1].strip() if ':' in feedback_line else ''\n",
    "                \n",
    "                self.node_performances[node_name]['score'] = (self.node_performances[node_name]['score'] + score) / 2\n",
    "                self.node_performances[node_name]['feedback'] += feedback + '\\n'\n",
    "            else:\n",
    "                print(f\"Warning: Could not find score or feedback for {node_name}\")\n",
    "\n",
    "    def update_node_functions(self):\n",
    "        print(\"Updating node functions...\")\n",
    "        for node_name, performance in self.node_performances.items():\n",
    "            if performance['score'] < 0.8:\n",
    "                print(f\"Updating function for node: {node_name}\")\n",
    "                updated_function = self.node_updater.update(\n",
    "                    node_name,\n",
    "                    self.node_functions[node_name],\n",
    "                    performance['score'],\n",
    "                    performance['feedback']\n",
    "                )\n",
    "                self.node_functions[node_name] = updated_function\n",
    "                print(f\"Updated {node_name}\")\n",
    "            else:\n",
    "                print(f\"Node {node_name} performance is satisfactory. No update needed.\")\n",
    "\n",
    "    def get_node_function(self, node_name: str) -> callable:\n",
    "        function_code = self.node_functions[node_name]\n",
    "        try:\n",
    "            exec(function_code, globals())\n",
    "            return eval(node_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing function for node {node_name}: {str(e)}\")\n",
    "            print(\"Function code:\")\n",
    "            print(function_code)\n",
    "            raise\n",
    "\n",
    "    def export_optimized_graph(self) -> StateGraph:\n",
    "        print(\"Exporting optimized graph...\")\n",
    "        optimized_graph = StateGraph(self.original_graph.state_type)\n",
    "        for node_name, node_func in self.node_functions.items():\n",
    "            optimized_graph.add_node(node_name, self.get_node_function(node_name))\n",
    "        \n",
    "        for edge in self.original_graph.edges:\n",
    "            optimized_graph.add_edge(edge.start, edge.end)\n",
    "        \n",
    "        for conditional_edge in self.original_graph.conditional_edges:\n",
    "            optimized_graph.add_conditional_edges(\n",
    "                conditional_edge.start,\n",
    "                conditional_edge.condition,\n",
    "                conditional_edge.edge_map\n",
    "            )\n",
    "        \n",
    "        optimized_graph.set_entry_point(self.original_graph.entry_point)\n",
    "        \n",
    "        print(\"Optimized graph exported.\")\n",
    "        return optimized_graph.compile()\n",
    "\n",
    "\n",
    "\n",
    "# llm_name = \"llama-3.1-8b-instant\"\n",
    "llm_name = \"llama-3.1-70b-versatile\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "optimizer = GraphAgentOptimizer(graph, ground_truth, llm)\n",
    "optimizer.load_graph()\n",
    "optimized_graph = optimizer.optimize(num_iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading graph<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading graph\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> nodes.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m1\u001b[0m nodes.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Iteration \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Executing node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Executing node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Forward pass completed.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Forward pass completed.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating output<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating output\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Warning: Could not find score or feedback for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Warning: Could not find score or feedback for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Executing node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Executing node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Forward pass completed.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Forward pass completed.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating output<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating output\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Warning: Could not find score or feedback for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Warning: Could not find score or feedback for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Executing node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Executing node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Forward pass completed.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Forward pass completed.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating output<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating output\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Warning: Could not find score or feedback for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Warning: Could not find score or feedback for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node functions<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node functions\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating function for node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating function for node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Input to ChatPromptTemplate is missing variables {\\'output\\', \\'\"write_email_output\"\\', \"state[\\'requirements\\']\", \\'\"input\"\\', \\'\"email\"\\', \\'input\\', \\'\\\\n                        \"input\"\\'}.  Expected: [\\'\\\\n                        \"input\"\\', \\'\"email\"\\', \\'\"input\"\\', \\'\"write_email_output\"\\', \\'input\\', \\'output\\', \"state[\\'requirements\\']\"] Received: [\\'original_function\\', \\'score\\', \\'feedback\\']'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 285\u001b[0m\n\u001b[1;32m    282\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GraphAgentOptimizer(original_graph, ground_truth, llm)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Get optimized graph\u001b[39;00m\n\u001b[1;32m    288\u001b[0m optimized_graph \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mexport_optimized_graph()\n",
      "Cell \u001b[0;32mIn[11], line 119\u001b[0m, in \u001b[0;36mGraphAgentOptimizer.optimize\u001b[0;34m(self, num_iterations)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing example \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_node_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 175\u001b[0m, in \u001b[0;36mGraphAgentOptimizer.update_node_functions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating function for node: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 175\u001b[0m     updated_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_updater\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperformance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeedback\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_functions[node_name] \u001b[38;5;241m=\u001b[39m updated_function\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mNodeFunctionUpdater.update\u001b[0;34m(self, node_name, original_function, score, feedback)\u001b[0m\n\u001b[1;32m     75\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt_template)\n\u001b[1;32m     76\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\n\u001b[0;32m---> 77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeedback\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeedback\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/runnables/base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/prompts/base.py:128\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    127\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/runnables/base.py:1509\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1506\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1507\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1508\u001b[0m         Output,\n\u001b[0;32m-> 1509\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1512\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1517\u001b[0m     )\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1519\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/runnables/config.py:346\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    345\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/prompts/base.py:111\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 111\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/site-packages/langchain_core/prompts/base.py:103\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    101\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inner_input\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Input to ChatPromptTemplate is missing variables {\\'output\\', \\'\"write_email_output\"\\', \"state[\\'requirements\\']\", \\'\"input\"\\', \\'\"email\"\\', \\'input\\', \\'\\\\n                        \"input\"\\'}.  Expected: [\\'\\\\n                        \"input\"\\', \\'\"email\"\\', \\'\"input\"\\', \\'\"write_email_output\"\\', \\'input\\', \\'output\\', \"state[\\'requirements\\']\"] Received: [\\'original_function\\', \\'score\\', \\'feedback\\']'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Dict, List, Any, Callable\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "import textwrap\n",
    "\n",
    "class NodeFunctionUpdater:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def update(self, node_name: str, original_function: str, score: float, feedback: str) -> str:\n",
    "        prompt_template = f\"\"\"\n",
    "        You are an expert in optimizing Python functions. I have a function that needs to be improved based on the \n",
    "        performance score and feedback provided. Please rewrite the function following the template provided below.\n",
    "        Original function:\n",
    "        ```\n",
    "        {original_function}\n",
    "        ```\n",
    "        Performance score: {score}\n",
    "        Feedback: {feedback}\n",
    "        \n",
    "        The improved function should follow this template:\n",
    "\n",
    "        def {node_name}(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            def updated_node() -> ChatPromptTemplate:\n",
    "                examples = [\n",
    "                    {{\n",
    "                        \"input\": textwrap.dedent('''\n",
    "                        [HERE GOES THE INPUT 1]\n",
    "                        '''),\n",
    "                        \"output\": textwrap.dedent('''\n",
    "                        [HERE GOES THE OUTPUT 1]            \n",
    "                        ''')\n",
    "                    }},\n",
    "                    {{\n",
    "                        \"input\": textwrap.dedent('''\n",
    "                        [HERE GOES THE INPUT 2]\n",
    "                        '''),\n",
    "                        \"output\": textwrap.dedent('''\n",
    "                        [HERE GOES THE OUTPUT 2] \n",
    "                        ''')\n",
    "                    }}\n",
    "                ]\n",
    "                \n",
    "                example_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"human\", \"{{input}}\"),\n",
    "                        (\"ai\", \"{{output}}\"),\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "                    example_prompt=example_prompt,\n",
    "                    examples=examples,\n",
    "                )\n",
    "                system_prompt = textwrap.dedent('''\n",
    "                [HERE GOES THE SYSTEM PROMPT]\n",
    "                ''')\n",
    "                final_prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", system_prompt),\n",
    "                        few_shot_prompt,\n",
    "                        (\"human\", \"{{input}}\"),\n",
    "                    ]\n",
    "                )\n",
    "                return final_prompt\n",
    "\n",
    "            prompt = updated_node()\n",
    "            chain = prompt | self.llm\n",
    "            result = chain.invoke({{\"input\": state['requirements']}})\n",
    "            return {{\"{node_name}_output\": result.content}}\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"original_function\": original_function,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "        return response.content\n",
    "\n",
    "class GraphAgentOptimizer:\n",
    "    def __init__(self, original_graph: StateGraph, ground_truth: List[Dict[str, Any]], llm: ChatGroq):\n",
    "        self.original_graph = original_graph\n",
    "        self.ground_truth = ground_truth\n",
    "        self.llm = llm\n",
    "        self.node_functions = {}\n",
    "        self.node_performances = {}\n",
    "        self.node_updater = NodeFunctionUpdater(llm)\n",
    "\n",
    "    def load_graph(self):\n",
    "        print(\"Loading graph...\")\n",
    "        for node_name, node_func in self.original_graph.nodes.items():\n",
    "            self.node_functions[node_name] = self.extract_function_source(node_func)\n",
    "            self.node_performances[node_name] = {'score': 0, 'feedback': ''}\n",
    "        print(f\"Loaded {len(self.node_functions)} nodes.\")\n",
    "\n",
    "    def extract_function_source(self, func):\n",
    "        if hasattr(func, 'func'):  # For RunnableCallable objects\n",
    "            return inspect.getsource(func.func)\n",
    "        elif callable(func):\n",
    "            return inspect.getsource(func)\n",
    "        else:\n",
    "            return str(func)  # Fallback for other types\n",
    "\n",
    "    def optimize(self, num_iterations: int):\n",
    "        self.load_graph()\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"Iteration {iteration + 1}\")\n",
    "            for i, example in enumerate(self.ground_truth):\n",
    "                print(f\"  Processing example {i + 1}\")\n",
    "                try:\n",
    "                    output = self.forward_pass(example['input'])\n",
    "                    self.evaluate_and_update(output, example['output'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i + 1}: {str(e)}\")\n",
    "            self.update_node_functions()\n",
    "\n",
    "    def forward_pass(self, input_data: str) -> Dict[str, Any]:\n",
    "        print(\"Performing forward pass...\")\n",
    "        state = {\"requirements\": input_data}\n",
    "        for node_name in self.original_graph.nodes:\n",
    "            print(f\"  Executing node: {node_name}\")\n",
    "            node_func = self.get_node_function(node_name)\n",
    "            state.update(node_func(state))\n",
    "        print(\"Forward pass completed.\")\n",
    "        return state\n",
    "\n",
    "    def evaluate_and_update(self, generated_output: Dict[str, Any], ground_truth_output: str):\n",
    "        print(\"Evaluating output...\")\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Compare the generated output with the ground truth output.\n",
    "        Provide a score from 0 to 1 for each node in the graph,\n",
    "        where 1 is perfect and 0 is completely wrong.\n",
    "        Also provide a brief explanation for each score.\n",
    "\n",
    "        Generated output:\n",
    "        {generated_output}\n",
    "\n",
    "        Ground truth output:\n",
    "        {ground_truth_output}\n",
    "\n",
    "        Response format:\n",
    "        [node_name] score: [score]\n",
    "        [node_name] feedback: [feedback]\n",
    "        (Repeat for each node)\n",
    "        \"\"\")\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"generated_output\": str(generated_output),\n",
    "            \"ground_truth_output\": ground_truth_output\n",
    "        })\n",
    "        \n",
    "        lines = response.content.split('\\n')\n",
    "        for node_name in self.node_functions.keys():\n",
    "            score_line = next((line for line in lines if f\"{node_name} score:\" in line), None)\n",
    "            feedback_line = next((line for line in lines if f\"{node_name} feedback:\" in line), None)\n",
    "            \n",
    "            if score_line and feedback_line:\n",
    "                score = float(score_line.split(':')[1].strip())\n",
    "                feedback = feedback_line.split(':', 1)[1].strip()\n",
    "                \n",
    "                self.node_performances[node_name]['score'] = (self.node_performances[node_name]['score'] + score) / 2\n",
    "                self.node_performances[node_name]['feedback'] += feedback + '\\n'\n",
    "            else:\n",
    "                print(f\"Warning: Could not find score or feedback for {node_name}\")\n",
    "\n",
    "    def update_node_functions(self):\n",
    "        print(\"Updating node functions...\")\n",
    "        for node_name, performance in self.node_performances.items():\n",
    "            if performance['score'] < 0.8:\n",
    "                print(f\"Updating function for node: {node_name}\")\n",
    "                updated_function = self.node_updater.update(\n",
    "                    node_name,\n",
    "                    self.node_functions[node_name],\n",
    "                    performance['score'],\n",
    "                    performance['feedback']\n",
    "                )\n",
    "                self.node_functions[node_name] = updated_function\n",
    "                print(f\"Updated {node_name}\")\n",
    "            else:\n",
    "                print(f\"Node {node_name} performance is satisfactory. No update needed.\")\n",
    "\n",
    "    def get_node_function(self, node_name: str) -> Callable:\n",
    "        function_code = self.node_functions[node_name]\n",
    "        try:\n",
    "            exec(function_code, globals())\n",
    "            return eval(node_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing function for node {node_name}: {str(e)}\")\n",
    "            print(\"Function code:\")\n",
    "            print(function_code)\n",
    "            raise\n",
    "\n",
    "    def export_optimized_graph(self) -> StateGraph:\n",
    "        print(\"Exporting optimized graph...\")\n",
    "        optimized_graph = StateGraph(self.original_graph.state_type)\n",
    "        for node_name, node_func in self.node_functions.items():\n",
    "            optimized_graph.add_node(node_name, self.get_node_function(node_name))\n",
    "        \n",
    "        for edge in self.original_graph.edges:\n",
    "            optimized_graph.add_edge(edge.start, edge.end)\n",
    "        \n",
    "        optimized_graph.set_entry_point(self.original_graph.entry_point)\n",
    "        \n",
    "        print(\"Optimized graph exported.\")\n",
    "        return optimized_graph.compile()\n",
    "\n",
    "# Example usage\n",
    "def write_email(state: Dict[str, str]) -> Dict[str, str]:\n",
    "    # Simplified email writing\n",
    "    email = f\"\"\"\n",
    "    Dear Client,\n",
    "    Based on your requirements: {state['requirements']}\n",
    "    We propose the following tasks:\n",
    "    1. Analyze requirements\n",
    "    2. Develop solution\n",
    "    3. Test and deploy\n",
    "    Best regards,\n",
    "    AI Team\n",
    "    \"\"\"\n",
    "    return {\"email\": email}\n",
    "\n",
    "# Create original graph\n",
    "original_graph = StateGraph(Dict[str, Any])\n",
    "original_graph.add_node(\"write_email\", write_email)\n",
    "original_graph.add_edge(\"write_email\", END)\n",
    "original_graph.set_entry_point(\"write_email\")\n",
    "\n",
    "# Ground truth data\n",
    "ground_truth = [\n",
    "    {\n",
    "        \"input\": \"Develop a website with user authentication\",\n",
    "        \"output\": \"\"\"\n",
    "        Dear Client,\n",
    "        Based on your requirements: Develop a website with user authentication\n",
    "        We propose the following tasks:\n",
    "        1. Design user authentication system\n",
    "        2. Develop frontend and backend\n",
    "        3. Implement security measures\n",
    "        4. Test and deploy website\n",
    "        Best regards,\n",
    "        AI Team\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Create a mobile app for task management\",\n",
    "        \"output\": \"\"\"\n",
    "        Dear Client,\n",
    "        Based on your requirements: Create a mobile app for task management\n",
    "        We propose the following tasks:\n",
    "        1. Design app UI/UX\n",
    "        2. Develop task management features\n",
    "        3. Implement data synchronization\n",
    "        4. Test on multiple devices\n",
    "        5. Deploy to app stores\n",
    "        Best regards,\n",
    "        AI Team\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Set up a data analytics pipeline\",\n",
    "        \"output\": \"\"\"\n",
    "        Dear Client,\n",
    "        Based on your requirements: Set up a data analytics pipeline\n",
    "        We propose the following tasks:\n",
    "        1. Analyze data sources and requirements\n",
    "        2. Design data pipeline architecture\n",
    "        3. Implement data collection and processing\n",
    "        4. Set up analytics and visualization tools\n",
    "        5. Test and optimize pipeline performance\n",
    "        Best regards,\n",
    "        AI Team\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize LLM and optimizer\n",
    "llm = ChatGroq(cache=False, temperature=0.2, model_name=\"llama-3.1-8b-instant\")\n",
    "optimizer = GraphAgentOptimizer(original_graph, ground_truth, llm)\n",
    "\n",
    "# Run optimization\n",
    "optimizer.optimize(num_iterations=3)\n",
    "\n",
    "# Get optimized graph\n",
    "optimized_graph = optimizer.export_optimized_graph()\n",
    "\n",
    "# Test the optimized graph\n",
    "test_input = \"Develop an e-commerce platform with payment integration\"\n",
    "result = optimized_graph.invoke({\"requirements\": test_input})\n",
    "print(\"\\nTest Result:\")\n",
    "print(result[\"email\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv(\"env\")\n",
    "\n",
    "llm_name = \"llama-3.1-70b-versatile\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    requirements: str\n",
    "    email: str\n",
    "\n",
    "def write_email(state: AgentState) -> AgentState:\n",
    "    # Hardcoded tasks\n",
    "    tasks = \"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\n",
    "    \n",
    "    # Simplified email writing\n",
    "    email = f\"\"\"\n",
    "    Dear Client,\n",
    "\n",
    "    Based on your requirements: {state['requirements']}\n",
    "\n",
    "    We propose the following tasks:\n",
    "    {tasks}\n",
    "\n",
    "    Best regards,\n",
    "    AI Team\n",
    "    \"\"\"\n",
    "    return {\"email\": email}\n",
    "\n",
    "# Initialize the graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"write_email\", write_email)\n",
    "graph.add_edge(\"write_email\", END)\n",
    "graph.set_entry_point(\"write_email\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "# Function to run the workflow\n",
    "def generate_email(requirements: str) -> str:\n",
    "    result = workflow.invoke({\"requirements\": requirements})\n",
    "    return result[\"email\"]\n",
    "\n",
    "requirements = \"Analyze customer data and provide recommendations\"\n",
    "email = generate_email(requirements)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write_email'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(graph.nodes.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Simplified email writing\n",
       "    email = f\"<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span>\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Simplified email writing\n",
       "    email = f\"\u001b[32m\"\"\u001b[0m\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get source code of the function\n",
    "print(inspect.getsource(graph.nodes[list(graph.nodes.keys())[0]].func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite_email\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/inspect.py:1262\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m \n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1262\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/inspect.py:1244\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 1244\u001b[0m lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mtb_frame\n",
      "File \u001b[0;32m~/mambaforge/envs/dspy/lib/python3.11/inspect.py:1081\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file)\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[0;32m-> 1081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not get source code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: could not get source code"
     ]
    }
   ],
   "source": [
    "inspect.getsource(graph.nodes[\"write_email\"].func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'write_email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'def write_email(state: Dict[str, str]) -&gt; Dict[str, str]:\\n    # Simplified email writing\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">email = f\"\"\"\\n    Dear Client,\\n    Based on your requirements: {state[\\'requirements\\']}\\n    We propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following tasks:\\n    1. Analyze requirements\\n    2. Develop solution\\n    3. Test and deploy\\n    Best regards,\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AI Team\\n    \"\"\"\\n    return {\"email\": email}\\n'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'write_email'\u001b[0m: \u001b[32m'def write_email\u001b[0m\u001b[32m(\u001b[0m\u001b[32mstate: Dict\u001b[0m\u001b[32m[\u001b[0m\u001b[32mstr, str\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -> Dict\u001b[0m\u001b[32m[\u001b[0m\u001b[32mstr, str\u001b[0m\u001b[32m]\u001b[0m\u001b[32m:\\n    # Simplified email writing\\n    \u001b[0m\n",
       "\u001b[32memail = f\"\"\"\\n    Dear Client,\\n    Based on your requirements: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mstate\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'requirements\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    We propose the \u001b[0m\n",
       "\u001b[32mfollowing tasks:\\n    1. Analyze requirements\\n    2. Develop solution\\n    3. Test and deploy\\n    Best regards,\\n\u001b[0m\n",
       "\u001b[32mAI Team\\n    \"\"\"\\n    return \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"email\": email\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(optimizer.node_functions['write_email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading graph<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading graph\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> nodes: write_email, generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m2\u001b[0m nodes: write_email, generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m1\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.6\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop a mobile app for task management'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear Client,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Based on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Develop a mobile app for task management'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear Client,\\n    \u001b[0m\n",
       "\u001b[32mBased on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "detailed tasks and structure present in the ground truth output. The email body is also incomplete and does not \n",
       "match the expected format.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a more complete email template with the client's requirements and a list\n",
       "of proposed tasks. However, the tasks are not as detailed or specific as in the ground truth output, and the email \n",
       "body is still missing some key elements, such as the numbered list of tasks and the closing sentence.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.5\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "detailed tasks and structure present in the ground truth output. The email body is also incomplete and does not \n",
       "match the expected format.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a more complete email template with the client's requirements and a list\n",
       "of proposed tasks. However, the tasks are not as detailed or specific as in the ground truth output, and the email \n",
       "body is still missing some key elements, such as the numbered list of tasks and the closing sentence.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create an e-commerce website with payment integration'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Client,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following tasks:\\n    {tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Create an e-commerce website with payment integration'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear \u001b[0m\n",
       "\u001b[32mClient,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the \u001b[0m\n",
       "\u001b[32mfollowing tasks:\\n    \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-generation\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"facebook/llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: \n",
       "<span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span> Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm_pipeline</span><span style=\"font-weight: bold\">(</span>prompt, <span style=\"color: #808000; text-decoration-color: #808000\">max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">][</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">})</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = \u001b[1;35mpipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"text-generation\"\u001b[0m, \u001b[33mmodel\u001b[0m=\u001b[32m\"facebook\u001b[0m\u001b[32m/llama-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \n",
       "\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm_pipeline\u001b[0m\u001b[1m(\u001b[0mprompt, \u001b[33mmax_length\u001b[0m=\u001b[1;36m200\u001b[0m\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'generated_text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-generation\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"facebook/llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: \n",
       "<span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span> Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm_pipeline</span><span style=\"font-weight: bold\">(</span>prompt, <span style=\"color: #808000; text-decoration-color: #808000\">max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">][</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "Note: The performance score is still <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> because the original code had no performance issues. The improved script is \n",
       "the same as the original code, with no changes made to the <span style=\"color: #008000; text-decoration-color: #008000\">'generate_email'</span> function. If you would like to improve \n",
       "the script further, please provide more information on what you would like to optimize.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = \u001b[1;35mpipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"text-generation\"\u001b[0m, \u001b[33mmodel\u001b[0m=\u001b[32m\"facebook\u001b[0m\u001b[32m/llama-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \n",
       "\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm_pipeline\u001b[0m\u001b[1m(\u001b[0mprompt, \u001b[33mmax_length\u001b[0m=\u001b[1;36m200\u001b[0m\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'generated_text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Note: The performance score is still \u001b[1;36m0\u001b[0m because the original code had no performance issues. The improved script is \n",
       "the same as the original code, with no changes made to the \u001b[32m'generate_email'\u001b[0m function. If you would like to improve \n",
       "the script further, please provide more information on what you would like to optimize.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m2\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: invalid syntax <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">string</span><span style=\"font-weight: bold\">&gt;</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error processing example \u001b[1;36m1\u001b[0m: invalid syntax \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mstring\u001b[0m\u001b[1m>\u001b[0m, line \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: invalid syntax <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">string</span><span style=\"font-weight: bold\">&gt;</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error processing example \u001b[1;36m2\u001b[0m: invalid syntax \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mstring\u001b[0m\u001b[1m>\u001b[0m, line \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">pipeline</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text-generation\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"facebook/llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm_pipeline</span><span style=\"font-weight: bold\">(</span>prompt, <span style=\"color: #808000; text-decoration-color: #808000\">max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span><span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">][</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    llm_pipeline = \u001b[1;35mpipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"text-generation\"\u001b[0m, \u001b[33mmodel\u001b[0m=\u001b[32m\"facebook\u001b[0m\u001b[32m/llama-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm_pipeline\u001b[0m\u001b[1m(\u001b[0mprompt, \u001b[33mmax_length\u001b[0m=\u001b[1;36m200\u001b[0m\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'generated_text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m3\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: invalid syntax <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">string</span><span style=\"font-weight: bold\">&gt;</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error processing example \u001b[1;36m1\u001b[0m: invalid syntax \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mstring\u001b[0m\u001b[1m>\u001b[0m, line \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: invalid syntax <span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">string</span><span style=\"font-weight: bold\">&gt;</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error processing example \u001b[1;36m2\u001b[0m: invalid syntax \u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mstring\u001b[0m\u001b[1m>\u001b[0m, line \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "from langchain.llms import LLM\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLM</span><span style=\"font-weight: bold\">(</span>llm_name, <span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.generate</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "from langchain.llms import LLM\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mLLM\u001b[0m\u001b[1m(\u001b[0mllm_name, \u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.generate\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain.llms import LLM\n",
       "from langchain.chat_models import ChatModel\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLM</span><span style=\"font-weight: bold\">(</span>llm_name, <span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    chat_model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatModel</span><span style=\"font-weight: bold\">(</span>llm<span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat_model.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain.llms import LLM\n",
       "from langchain.chat_models import ChatModel\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mLLM\u001b[0m\u001b[1m(\u001b[0mllm_name, \u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    chat_model = \u001b[1;35mChatModel\u001b[0m\u001b[1m(\u001b[0mllm\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mchat_model.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Optimized code:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Optimized code:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain.llms import LLM\n",
       "from langchain.chat_models import ChatModel\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLM</span><span style=\"font-weight: bold\">(</span>llm_name, <span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Use LLM to generate email\n",
       "    chat_model = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatModel</span><span style=\"font-weight: bold\">(</span>llm<span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">chat_model.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    state = <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">}</span>\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">(</span>state<span style=\"font-weight: bold\">)</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain.llms import LLM\n",
       "from langchain.chat_models import ChatModel\n",
       "from transformers import pipeline\n",
       "import torch\n",
       "\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mLLM\u001b[0m\u001b[1m(\u001b[0mllm_name, \u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Use LLM to generate email\n",
       "    chat_model = \u001b[1;35mChatModel\u001b[0m\u001b[1m(\u001b[0mllm\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. \n",
       "Analyze requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mchat_model.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m'text'\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    state = \u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0mstate\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Optimized graph result:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Optimized graph result:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "    Based on your requirements: Design a customer loyalty program for a retail chain\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "    Based on your requirements: Design a customer loyalty program for a retail chain\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def escape_curly_braces(text: str) -> str:\n",
    "    text = str(text)\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "\n",
    "class NodeFunctionUpdater:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def update(self, node_name: str, original_code: str, score: float, feedback: str) -> str:\n",
    "        print(f\"Updating node: {node_name}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        prompt_template = f\"\"\"\n",
    "        You are an expert in optimizing Python code. Improve the following script, focusing on the '{node_name}' function.\n",
    "        The improved function should use an LLM for generating responses.\n",
    "\n",
    "        Original code:\n",
    "        ```python\n",
    "        {original_code}\n",
    "        ```\n",
    "        Performance score: {score}\n",
    "        Feedback: {feedback}\n",
    "\n",
    "        Please rewrite the entire script, making sure to:\n",
    "        1. Keep all necessary imports.\n",
    "        2. Improve the '{node_name}' function to use an LLM for generating responses.\n",
    "        3. Maintain the overall structure of the script, including the graph setup and workflow compilation.\n",
    "        4. Ensure that the improved script is fully functional and can be executed as is.\n",
    "\n",
    "        Return the entire improved Python script, without any additional explanation.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = escape_curly_braces(prompt_template)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"original_code\": original_code,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "        print(\"LLM response content:\")\n",
    "        print(response.content)\n",
    "        return response.content\n",
    "\n",
    "class GraphAgentOptimizer:\n",
    "    def __init__(self, original_code: str, ground_truth: List[Dict[str, Any]], llm):\n",
    "        self.original_code = original_code\n",
    "        self.ground_truth = ground_truth\n",
    "        self.llm = llm\n",
    "        self.node_performances = {}\n",
    "        self.updater = NodeFunctionUpdater(llm)\n",
    "        self.optimized_code = original_code\n",
    "\n",
    "    def load_graph(self):\n",
    "        print(\"Loading graph...\")\n",
    "        function_pattern = re.compile(r'def\\s+(\\w+)\\s*\\(')\n",
    "        self.node_names = function_pattern.findall(self.original_code)\n",
    "        for node_name in self.node_names:\n",
    "            self.node_performances[node_name] = {'score': 0, 'feedback': ''}\n",
    "        print(f\"Loaded {len(self.node_names)} nodes: {', '.join(self.node_names)}\")\n",
    "\n",
    "    def optimize(self, num_iterations: int):\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"\\nIteration {iteration + 1}/{num_iterations}\")\n",
    "            for i, example in enumerate(self.ground_truth):\n",
    "                print(f\"  Processing example {i + 1}/{len(self.ground_truth)}\")\n",
    "                try:\n",
    "                    output = self.forward_pass(example['input'])\n",
    "                    self.evaluate_and_update(output, example['output'])\n",
    "                    # print generated output\n",
    "                    print(f\"Generated output: {output}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i + 1}: {str(e)}\")\n",
    "            self.update_code()\n",
    "\n",
    "    def forward_pass(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"Performing forward pass...\")\n",
    "        locals_dict = {}\n",
    "        exec(self.optimized_code, globals(), locals_dict)\n",
    "        workflow = locals_dict['workflow']\n",
    "        return workflow.invoke(input_data)\n",
    "\n",
    "    def evaluate_and_update(self, generated_output: Dict[str, Any], ground_truth_output: Dict[str, Any]):\n",
    "        print(\"Evaluating and updating node performances...\")\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Compare the generated output with the ground truth output.\n",
    "        Provide a score from 0 to 1 for each function in the script,\n",
    "        where 1 is perfect and 0 is completely wrong.\n",
    "        Also provide a brief explanation for each score.\n",
    "\n",
    "        Generated output:\n",
    "        {generated_output}\n",
    "\n",
    "        Ground truth output:\n",
    "        {ground_truth_output}\n",
    "\n",
    "        Functions to evaluate: {node_names}\n",
    "\n",
    "        Response format:\n",
    "        [function_name] score: [score]\n",
    "        [function_name] feedback: [feedback]\n",
    "        (Repeat for each function)\n",
    "        \"\"\")\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"generated_output\": str(generated_output),\n",
    "            \"ground_truth_output\": str(ground_truth_output),\n",
    "            \"node_names\": \", \".join(self.node_names)\n",
    "        })\n",
    "\n",
    "        print(\"Raw LLM response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        self.parse_llm_response(response.content)\n",
    "\n",
    "    def parse_llm_response(self, response: str):\n",
    "        lines = response.split('\\n')\n",
    "        current_node = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            score_match = re.match(r'(\\w+)\\s+score:\\s*([\\d.]+)', line, re.IGNORECASE)\n",
    "            if score_match:\n",
    "                current_node = score_match.group(1)\n",
    "                score = float(score_match.group(2))\n",
    "                if current_node in self.node_performances:\n",
    "                    self.node_performances[current_node]['score'] = (self.node_performances[current_node]['score'] + score) / 2\n",
    "                    print(f\"Updated score for {current_node}: {score}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected node {current_node}\")\n",
    "            elif current_node and 'feedback' in line.lower():\n",
    "                feedback = line.split(':', 1)[1].strip() if ':' in line else line\n",
    "                if current_node in self.node_performances:\n",
    "                    self.node_performances[current_node]['feedback'] += feedback + '\\n'\n",
    "                    print(f\"Updated feedback for {current_node}: {feedback}\")\n",
    "\n",
    "    def update_code(self):\n",
    "        print(\"Updating code...\")\n",
    "        for node_name, performance in self.node_performances.items():\n",
    "            if performance['score'] < 0.8:\n",
    "                try:\n",
    "                    self.optimized_code = self.updater.update(\n",
    "                        node_name,\n",
    "                        self.optimized_code,\n",
    "                        performance['score'],\n",
    "                        performance['feedback']\n",
    "                    )\n",
    "                    print(f\"Updated code for {node_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating code for {node_name}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Node {node_name} performance is good (score: {performance['score']}). Skipping update.\")\n",
    "\n",
    "    def get_optimized_code(self) -> str:\n",
    "        return self.optimized_code\n",
    "\n",
    "# Usage\n",
    "llm_name = \"llama-3.1-8b-instant\"\n",
    "# llm_name = \"llama-3.1-70b-versatile\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "\n",
    "original_code = \"\"\"\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv(\"env\")\n",
    "llm_name = \"llama-3.1-8b-instant\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    requirements: str\n",
    "    email: str\n",
    "\n",
    "def write_email(state: AgentState) -> AgentState:\n",
    "    # Hardcoded tasks\n",
    "    tasks = \"1. Analyze requirements\\\\n2. Develop solution\\\\n3. Test and deploy\"\n",
    "    \n",
    "    # Simplified email writing\n",
    "    email = f'''\n",
    "    Dear Client,\n",
    "    Based on your requirements: {state['requirements']}\n",
    "    We propose the following tasks:\n",
    "    {{tasks}}\n",
    "    Best regards,\n",
    "    AI Team\n",
    "    '''\n",
    "    return {\"email\": email}\n",
    "\n",
    "# Initialize the graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"write_email\", write_email)\n",
    "graph.add_edge(\"write_email\", END)\n",
    "graph.set_entry_point(\"write_email\")\n",
    "workflow = graph.compile()\n",
    "\n",
    "# Function to run the workflow\n",
    "def generate_email(requirements: str) -> str:\n",
    "    result = workflow.invoke({\"requirements\": requirements})\n",
    "    return result[\"email\"]\n",
    "\n",
    "requirements = \"Analyze customer data and provide recommendations\"\n",
    "email = generate_email(requirements)\n",
    "print(email)\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = [\n",
    "    {\n",
    "        \"input\": {\"requirements\": \"Develop a mobile app for task management\"},\n",
    "        \"output\": {\n",
    "            \"email\": \"\"\"\n",
    "            Dear Client,\n",
    "            Based on your requirements: Develop a mobile app for task management\n",
    "            We propose the following tasks:\n",
    "            1. Analyze user requirements and define app features\n",
    "            2. Design intuitive UI/UX for task management\n",
    "            3. Develop core functionality (task creation, editing, deletion)\n",
    "            4. Implement data synchronization and cloud storage\n",
    "            5. Conduct thorough testing on multiple devices\n",
    "            6. Deploy to app stores and gather user feedback\n",
    "            Best regards,\n",
    "            AI Team\n",
    "            \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"requirements\": \"Create an e-commerce website with payment integration\"},\n",
    "        \"output\": {\n",
    "            \"email\": \"\"\"\n",
    "            Dear Client,\n",
    "            Based on your requirements: Create an e-commerce website with payment integration\n",
    "            We propose the following tasks:\n",
    "            1. Analyze business requirements and select appropriate e-commerce platform\n",
    "            2. Design responsive and user-friendly website layout\n",
    "            3. Set up product catalog and inventory management system\n",
    "            4. Implement secure payment gateway integration\n",
    "            5. Develop order processing and shipment tracking features\n",
    "            6. Conduct thorough security and performance testing\n",
    "            7. Launch website and provide post-launch support\n",
    "            Best regards,\n",
    "            AI Team\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = GraphAgentOptimizer(original_code, ground_truth, llm)\n",
    "optimizer.load_graph()\n",
    "optimizer.optimize(num_iterations=3)\n",
    "optimized_code = optimizer.get_optimized_code()\n",
    "\n",
    "print(\"Optimized code:\")\n",
    "print(optimized_code)\n",
    "\n",
    "# Test the optimized code\n",
    "# exec(optimized_code, globals())\n",
    "test_input = {\"requirements\": \"Design a customer loyalty program for a retail chain\"}\n",
    "result = generate_email(test_input[\"requirements\"])\n",
    "print(\"Optimized graph result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading graph<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading graph\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> nodes: write_email, generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m2\u001b[0m nodes: write_email, generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m1\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.6\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop a mobile app for task management'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear Client,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Based on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Develop a mobile app for task management'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear Client,\\n    \u001b[0m\n",
       "\u001b[32mBased on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>**\n",
       "write_email feedback: The function is able to generate a basic email template with the client's requirements, but \n",
       "it lacks the specific tasks that should be included in the email. The tasks are crucial for a comprehensive \n",
       "e-commerce website development project.\n",
       "\n",
       "**generate_email score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>**\n",
       "generate_email feedback: The function is able to generate a well-structured email with the client's requirements \n",
       "and a brief introduction. However, it fails to include the specific tasks that should be included in the email, \n",
       "which is a critical component of the email. The tasks are missing, and the email seems incomplete.\n",
       "\n",
       "Note that the scores are subjective and based on the evaluation of the generated output compared to the ground \n",
       "truth output. The scores can be adjusted based on the specific requirements and expectations of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email score: \u001b[1;36m0.5\u001b[0m**\n",
       "write_email feedback: The function is able to generate a basic email template with the client's requirements, but \n",
       "it lacks the specific tasks that should be included in the email. The tasks are crucial for a comprehensive \n",
       "e-commerce website development project.\n",
       "\n",
       "**generate_email score: \u001b[1;36m0.8\u001b[0m**\n",
       "generate_email feedback: The function is able to generate a well-structured email with the client's requirements \n",
       "and a brief introduction. However, it fails to include the specific tasks that should be included in the email, \n",
       "which is a critical component of the email. The tasks are missing, and the email seems incomplete.\n",
       "\n",
       "Note that the scores are subjective and based on the evaluation of the generated output compared to the ground \n",
       "truth output. The scores can be adjusted based on the specific requirements and expectations of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create an e-commerce website with payment integration'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Client,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following tasks:\\n    {tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Create an e-commerce website with payment integration'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear \u001b[0m\n",
       "\u001b[32mClient,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the \u001b[0m\n",
       "\u001b[32mfollowing tasks:\\n    \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: \n",
       "<span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span> Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">})</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \n",
       "\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m\"text\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>requirements<span style=\"font-weight: bold\">}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    return response\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[32m\"llama\u001b[0m\u001b[32m-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mrequirements\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    return response\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m2\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.6\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop a mobile app for task management'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear Client,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Based on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Develop a mobile app for task management'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear Client,\\n    \u001b[0m\n",
       "\u001b[32mBased on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that should be included in the email. The tasks are crucial for a comprehensive e-commerce website \n",
       "development project.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a well-structured email with the client's requirements and a brief \n",
       "introduction. However, it fails to include the specific tasks that should be included in the email, which is a \n",
       "critical component of the email. The tasks are missing, and the email seems incomplete without them.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.5\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that should be included in the email. The tasks are crucial for a comprehensive e-commerce website \n",
       "development project.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a well-structured email with the client's requirements and a brief \n",
       "introduction. However, it fails to include the specific tasks that should be included in the email, which is a \n",
       "critical component of the email. The tasks are missing, and the email seems incomplete without them.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create an e-commerce website with payment integration'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Client,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following tasks:\\n    {tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Create an e-commerce website with payment integration'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear \u001b[0m\n",
       "\u001b[32mClient,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the \u001b[0m\n",
       "\u001b[32mfollowing tasks:\\n    \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: \n",
       "<span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span> Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">})</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \n",
       "\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m\"text\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>requirements<span style=\"font-weight: bold\">}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    return response\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[32m\"llama\u001b[0m\u001b[32m-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mrequirements\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    return response\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Iteration <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Iteration \u001b[1;36m3\u001b[0m/\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m1\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span>\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email**\n",
       "score: \u001b[1;36m0.8\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it lacks the \n",
       "specific tasks that were proposed in the ground truth output. The tasks are also not formatted correctly, with no \n",
       "numbering or bullet points.\n",
       "\n",
       "**generate_email**\n",
       "score: \u001b[1;36m0.6\u001b[0m\n",
       "feedback: The function is able to generate a basic email template with the client's requirements, but it fails to \n",
       "include the proposed tasks that were expected. The email also lacks a clear structure and formatting, making it \n",
       "difficult to read and understand.\n",
       "\n",
       "Note that the scores are subjective and based on the specific requirements of the ground truth output. The scores \n",
       "can be adjusted based on the specific requirements of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Develop a mobile app for task management'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear Client,\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Based on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Develop a mobile app for task management'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear Client,\\n    \u001b[0m\n",
       "\u001b[32mBased on your requirements: Develop a mobile app for task management\\n    We propose the following tasks:\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Processing example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Processing example \u001b[1;36m2\u001b[0m/\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performing forward pass<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performing forward pass\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze requirements\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Develop solution\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "\n",
       "    We propose the following tasks:\n",
       "    \u001b[1;36m1\u001b[0m. Analyze requirements\n",
       "\u001b[1;36m2\u001b[0m. Develop solution\n",
       "\u001b[1;36m3\u001b[0m. Test and deploy\n",
       "\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating and updating node performances<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluating and updating node performances\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Raw LLM response:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Raw LLM response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>**\n",
       "write_email feedback: The function is able to generate a basic email template with the client's requirements, but \n",
       "it lacks the specific tasks that should be included in the email. The tasks are crucial for a comprehensive \n",
       "e-commerce website development project.\n",
       "\n",
       "**generate_email score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>**\n",
       "generate_email feedback: The function is able to generate a well-structured email with the client's requirements \n",
       "and a brief introduction. However, it fails to include the specific tasks that should be included in the email, \n",
       "which is a critical component of the email. The tasks are missing, and the email seems incomplete.\n",
       "\n",
       "Note that the scores are subjective and based on the evaluation of the generated output compared to the ground \n",
       "truth output. The scores can be adjusted based on the specific requirements and expectations of the project.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here is the evaluation of the generated output compared to the ground truth output:\n",
       "\n",
       "**write_email score: \u001b[1;36m0.5\u001b[0m**\n",
       "write_email feedback: The function is able to generate a basic email template with the client's requirements, but \n",
       "it lacks the specific tasks that should be included in the email. The tasks are crucial for a comprehensive \n",
       "e-commerce website development project.\n",
       "\n",
       "**generate_email score: \u001b[1;36m0.8\u001b[0m**\n",
       "generate_email feedback: The function is able to generate a well-structured email with the client's requirements \n",
       "and a brief introduction. However, it fails to include the specific tasks that should be included in the email, \n",
       "which is a critical component of the email. The tasks are missing, and the email seems incomplete.\n",
       "\n",
       "Note that the scores are subjective and based on the evaluation of the generated output compared to the ground \n",
       "truth output. The scores can be adjusted based on the specific requirements and expectations of the project.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated output: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create an e-commerce website with payment integration'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'email'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\n    Dear </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Client,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following tasks:\\n    {tasks}\\n    Best regards,\\n    AI Team\\n    '</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated output: \u001b[1m{\u001b[0m\u001b[32m'requirements'\u001b[0m: \u001b[32m'Create an e-commerce website with payment integration'\u001b[0m, \u001b[32m'email'\u001b[0m: \u001b[32m'\\n    Dear \u001b[0m\n",
       "\u001b[32mClient,\\n    Based on your requirements: Create an e-commerce website with payment integration\\n    We propose the \u001b[0m\n",
       "\u001b[32mfollowing tasks:\\n    \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtasks\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    Best regards,\\n    AI Team\\n    '\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating code<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating code\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span> We propose the following tasks: \n",
       "<span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span> Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    email = response<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"text\"</span><span style=\"font-weight: bold\">]</span>\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">})</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Use LLM to generate email\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m We propose the following tasks: \n",
       "\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    email = response\u001b[1m[\u001b[0m\u001b[32m\"text\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for write_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for write_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating node: generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating node: generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Feedback: \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Feedback: \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM response content:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM response content:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">```python\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span><span style=\"font-weight: bold\">)</span>\n",
       "    prompt = f\"Dear Client, Based on your requirements: <span style=\"font-weight: bold\">{</span>requirements<span style=\"font-weight: bold\">}</span> We propose the following tasks: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llm.call</span><span style=\"font-weight: bold\">(</span>prompt<span style=\"font-weight: bold\">)</span>\n",
       "    return response\n",
       "```\n",
       "</pre>\n"
      ],
      "text/plain": [
       "```python\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[32m\"llama\u001b[0m\u001b[32m-3.1-8b-instant\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "    prompt = f\"Dear Client, Based on your requirements: \u001b[1m{\u001b[0mrequirements\u001b[1m}\u001b[0m We propose the following tasks: \u001b[1;36m1\u001b[0m. Analyze \n",
       "requirements\\n2. Develop solution\\n3. Test and deploy Best regards, AI Team\"\n",
       "    response = \u001b[1;35mllm.call\u001b[0m\u001b[1m(\u001b[0mprompt\u001b[1m)\u001b[0m\n",
       "    return response\n",
       "```\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updated code for generate_email\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updated code for generate_email\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Optimized code:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Optimized code:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load_dotenv</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"env\"</span><span style=\"font-weight: bold\">)</span>\n",
       "llm_name = <span style=\"color: #008000; text-decoration-color: #008000\">\"llama-3.1-8b-instant\"</span>\n",
       "llm = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatGroq</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">cache</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_name</span>=<span style=\"color: #800080; text-decoration-color: #800080\">llm_name</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AgentState</span><span style=\"font-weight: bold\">(</span>TypedDict<span style=\"font-weight: bold\">)</span>:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">write_email</span><span style=\"font-weight: bold\">(</span>state: AgentState<span style=\"font-weight: bold\">)</span> -&gt; AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = <span style=\"color: #008000; text-decoration-color: #008000\">\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"</span>\n",
       "    \n",
       "    # Simplified email writing\n",
       "    email = f'<span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "    Dear Client,\n",
       "    Based on your requirements: <span style=\"font-weight: bold\">{</span>state<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'requirements'</span><span style=\"font-weight: bold\">]}</span>\n",
       "    We propose the following tasks:\n",
       "    <span style=\"font-weight: bold\">{{</span>tasks<span style=\"font-weight: bold\">}}</span>\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">''</span>'\n",
       "    return <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span>: email<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "# Initialize the graph\n",
       "graph = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StateGraph</span><span style=\"font-weight: bold\">(</span>AgentState<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_node</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, write_email<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.add_edge</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span>, END<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.set_entry_point</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"write_email\"</span><span style=\"font-weight: bold\">)</span>\n",
       "workflow = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">graph.compile</span><span style=\"font-weight: bold\">()</span>\n",
       "\n",
       "# Function to run the workflow\n",
       "def <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements: str<span style=\"font-weight: bold\">)</span> -&gt; str:\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">workflow.invoke</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">\"requirements\"</span>: requirements<span style=\"font-weight: bold\">})</span>\n",
       "    return result<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"email\"</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "requirements = <span style=\"color: #008000; text-decoration-color: #008000\">\"Analyze customer data and provide recommendations\"</span>\n",
       "email = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">generate_email</span><span style=\"font-weight: bold\">(</span>requirements<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>email<span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "from typing import TypedDict\n",
       "from langgraph.graph import StateGraph, END\n",
       "from dotenv import load_dotenv\n",
       "from langchain_groq import ChatGroq\n",
       "\u001b[1;35mload_dotenv\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"env\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "llm_name = \u001b[32m\"llama-3.1-8b-instant\"\u001b[0m\n",
       "llm = \u001b[1;35mChatGroq\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcache\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33mmodel_name\u001b[0m=\u001b[35mllm_name\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "class \u001b[1;35mAgentState\u001b[0m\u001b[1m(\u001b[0mTypedDict\u001b[1m)\u001b[0m:\n",
       "    requirements: str\n",
       "    email: str\n",
       "\n",
       "def \u001b[1;35mwrite_email\u001b[0m\u001b[1m(\u001b[0mstate: AgentState\u001b[1m)\u001b[0m -> AgentState:\n",
       "    # Hardcoded tasks\n",
       "    tasks = \u001b[32m\"1. Analyze requirements\\n2. Develop solution\\n3. Test and deploy\"\u001b[0m\n",
       "    \n",
       "    # Simplified email writing\n",
       "    email = f'\u001b[32m''\u001b[0m\n",
       "    Dear Client,\n",
       "    Based on your requirements: \u001b[1m{\u001b[0mstate\u001b[1m[\u001b[0m\u001b[32m'requirements'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "    We propose the following tasks:\n",
       "    \u001b[1m{\u001b[0m\u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \u001b[32m''\u001b[0m'\n",
       "    return \u001b[1m{\u001b[0m\u001b[32m\"email\"\u001b[0m: email\u001b[1m}\u001b[0m\n",
       "\n",
       "# Initialize the graph\n",
       "graph = \u001b[1;35mStateGraph\u001b[0m\u001b[1m(\u001b[0mAgentState\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_node\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, write_email\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.add_edge\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m, END\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mgraph.set_entry_point\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"write_email\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "workflow = \u001b[1;35mgraph.compile\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "# Function to run the workflow\n",
       "def \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements: str\u001b[1m)\u001b[0m -> str:\n",
       "    result = \u001b[1;35mworkflow.invoke\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m\"requirements\"\u001b[0m: requirements\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    return result\u001b[1m[\u001b[0m\u001b[32m\"email\"\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n",
       "requirements = \u001b[32m\"Analyze customer data and provide recommendations\"\u001b[0m\n",
       "email = \u001b[1;35mgenerate_email\u001b[0m\u001b[1m(\u001b[0mrequirements\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0memail\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    <span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span>\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "    Based on your requirements: Analyze customer data and provide recommendations\n",
       "    We propose the following tasks:\n",
       "    \u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Optimized graph result:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Optimized graph result:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Dear Client,\n",
       "    Based on your requirements: Design a customer loyalty program for a retail chain\n",
       "    We propose the following tasks:\n",
       "    <span style=\"font-weight: bold\">{</span>tasks<span style=\"font-weight: bold\">}</span>\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Dear Client,\n",
       "    Based on your requirements: Design a customer loyalty program for a retail chain\n",
       "    We propose the following tasks:\n",
       "    \u001b[1m{\u001b[0mtasks\u001b[1m}\u001b[0m\n",
       "    Best regards,\n",
       "    AI Team\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def escape_curly_braces(text: str) -> str:\n",
    "    text = str(text)\n",
    "    return text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "def replace_function_in_source(original_source: str, old_function_name: str, new_function_code: str) -> str:\n",
    "    pattern = re.compile(rf\"def {old_function_name}\\(.*?\\):.*?(\\n    .*)*?\\n\", re.DOTALL)\n",
    "    modified_source = re.sub(pattern, new_function_code, original_source)\n",
    "    return modified_source\n",
    "\n",
    "class NodeFunctionUpdater:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def update(self, node_name: str, original_code: str, score: float, feedback: str) -> str:\n",
    "        print(f\"Updating node: {node_name}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        prompt_template = f\"\"\"\n",
    "        You are an expert in optimizing Python code. Improve the following function, focusing on the '{node_name}' function.\n",
    "        The improved function should use an LLM for generating responses.\n",
    "\n",
    "        Original code:\n",
    "        ```python\n",
    "        {original_code}\n",
    "        ```\n",
    "        Performance score: {score}\n",
    "        Feedback: {feedback}\n",
    "\n",
    "        Please rewrite only the '{node_name}' function, making sure to:\n",
    "        1. Improve the function to use an LLM for generating responses.\n",
    "        2. Ensure that the improved function is fully functional and can be executed as is.\n",
    "        3. Maintain the same function signature and return type.\n",
    "\n",
    "        Return only the improved Python function, without any additional explanation.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_template = escape_curly_braces(prompt_template)\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"original_code\": original_code,\n",
    "            \"score\": score,\n",
    "            \"feedback\": feedback\n",
    "        })\n",
    "        print(\"LLM response content:\")\n",
    "        print(response.content)\n",
    "        return response.content\n",
    "\n",
    "class GraphAgentOptimizer:\n",
    "    def __init__(self, original_code: str, ground_truth: List[Dict[str, Any]], llm):\n",
    "        self.original_code = original_code\n",
    "        self.ground_truth = ground_truth\n",
    "        self.llm = llm\n",
    "        self.node_performances = {}\n",
    "        self.updater = NodeFunctionUpdater(llm)\n",
    "        self.optimized_code = original_code\n",
    "\n",
    "    def load_graph(self):\n",
    "        print(\"Loading graph...\")\n",
    "        function_pattern = re.compile(r'def\\s+(\\w+)\\s*\\(')\n",
    "        self.node_names = function_pattern.findall(self.original_code)\n",
    "        for node_name in self.node_names:\n",
    "            self.node_performances[node_name] = {'score': 0, 'feedback': ''}\n",
    "        print(f\"Loaded {len(self.node_names)} nodes: {', '.join(self.node_names)}\")\n",
    "\n",
    "    def optimize(self, num_iterations: int):\n",
    "        for iteration in range(num_iterations):\n",
    "            print(f\"\\nIteration {iteration + 1}/{num_iterations}\")\n",
    "            for i, example in enumerate(self.ground_truth):\n",
    "                print(f\"  Processing example {i + 1}/{len(self.ground_truth)}\")\n",
    "                try:\n",
    "                    output = self.forward_pass(example['input'])\n",
    "                    self.evaluate_and_update(output, example['output'])\n",
    "                    print(f\"Generated output: {output}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i + 1}: {str(e)}\")\n",
    "            self.update_code()\n",
    "\n",
    "    def forward_pass(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"Performing forward pass...\")\n",
    "        locals_dict = {}\n",
    "        exec(self.optimized_code, globals(), locals_dict)\n",
    "        workflow = locals_dict['workflow']\n",
    "        return workflow.invoke(input_data)\n",
    "\n",
    "    def evaluate_and_update(self, generated_output: Dict[str, Any], ground_truth_output: Dict[str, Any]):\n",
    "        print(\"Evaluating and updating node performances...\")\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Compare the generated output with the ground truth output.\n",
    "        Provide a score from 0 to 1 for each function in the script,\n",
    "        where 1 is perfect and 0 is completely wrong.\n",
    "        Also provide a brief explanation for each score.\n",
    "\n",
    "        Generated output:\n",
    "        {generated_output}\n",
    "\n",
    "        Ground truth output:\n",
    "        {ground_truth_output}\n",
    "\n",
    "        Functions to evaluate: {node_names}\n",
    "\n",
    "        Response format:\n",
    "        [function_name] score: [score]\n",
    "        [function_name] feedback: [feedback]\n",
    "        (Repeat for each function)\n",
    "        \"\"\")\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"generated_output\": str(generated_output),\n",
    "            \"ground_truth_output\": str(ground_truth_output),\n",
    "            \"node_names\": \", \".join(self.node_names)\n",
    "        })\n",
    "\n",
    "        print(\"Raw LLM response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        self.parse_llm_response(response.content)\n",
    "\n",
    "    def parse_llm_response(self, response: str):\n",
    "        lines = response.split('\\n')\n",
    "        current_node = None\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            score_match = re.match(r'(\\w+)\\s+score:\\s*([\\d.]+)', line, re.IGNORECASE)\n",
    "            if score_match:\n",
    "                current_node = score_match.group(1)\n",
    "                score = float(score_match.group(2))\n",
    "                if current_node in self.node_performances:\n",
    "                    self.node_performances[current_node]['score'] = (self.node_performances[current_node]['score'] + score) / 2\n",
    "                    print(f\"Updated score for {current_node}: {score}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected node {current_node}\")\n",
    "            elif current_node and 'feedback' in line.lower():\n",
    "                feedback = line.split(':', 1)[1].strip() if ':' in line else line\n",
    "                if current_node in self.node_performances:\n",
    "                    self.node_performances[current_node]['feedback'] += feedback + '\\n'\n",
    "                    print(f\"Updated feedback for {current_node}: {feedback}\")\n",
    "\n",
    "    def update_code(self):\n",
    "        print(\"Updating code...\")\n",
    "        for node_name, performance in self.node_performances.items():\n",
    "            if performance['score'] < 0.8:\n",
    "                try:\n",
    "                    new_function_code = self.updater.update(\n",
    "                        node_name,\n",
    "                        self.optimized_code,\n",
    "                        performance['score'],\n",
    "                        performance['feedback']\n",
    "                    )\n",
    "                    self.optimized_code = replace_function_in_source(self.optimized_code, node_name, new_function_code)\n",
    "                    print(f\"Updated code for {node_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating code for {node_name}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Node {node_name} performance is good (score: {performance['score']}). Skipping update.\")\n",
    "\n",
    "    def get_optimized_code(self) -> str:\n",
    "        return self.optimized_code\n",
    "\n",
    "# Usage example\n",
    "llm_name = \"llama-3.1-8b-instant\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "\n",
    "original_code = \"\"\"\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv(\"env\")\n",
    "llm_name = \"llama-3.1-8b-instant\"\n",
    "llm = ChatGroq(cache=False, temperature=0.0, model_name=llm_name)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    requirements: str\n",
    "    email: str\n",
    "\n",
    "def write_email(state: AgentState) -> AgentState:\n",
    "    # Hardcoded tasks\n",
    "    tasks = \"1. Analyze requirements\\\\n2. Develop solution\\\\n3. Test and deploy\"\n",
    "    \n",
    "    # Simplified email writing\n",
    "    email = f'''\n",
    "    Dear Client,\n",
    "    Based on your requirements: {state['requirements']}\n",
    "    We propose the following tasks:\n",
    "    {{tasks}}\n",
    "    Best regards,\n",
    "    AI Team\n",
    "    '''\n",
    "    return {\"email\": email}\n",
    "\n",
    "# Initialize the graph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"write_email\", write_email)\n",
    "graph.add_edge(\"write_email\", END)\n",
    "graph.set_entry_point(\"write_email\")\n",
    "workflow = graph.compile()\n",
    "\n",
    "# Function to run the workflow\n",
    "def generate_email(requirements: str) -> str:\n",
    "    result = workflow.invoke({\"requirements\": requirements})\n",
    "    return result[\"email\"]\n",
    "\n",
    "requirements = \"Analyze customer data and provide recommendations\"\n",
    "email = generate_email(requirements)\n",
    "print(email)\n",
    "\"\"\"\n",
    "\n",
    "ground_truth = [\n",
    "    {\n",
    "        \"input\": {\"requirements\": \"Develop a mobile app for task management\"},\n",
    "        \"output\": {\n",
    "            \"email\": \"\"\"\n",
    "            Dear Client,\n",
    "            Based on your requirements: Develop a mobile app for task management\n",
    "            We propose the following tasks:\n",
    "            1. Analyze user requirements and define app features\n",
    "            2. Design intuitive UI/UX for task management\n",
    "            3. Develop core functionality (task creation, editing, deletion)\n",
    "            4. Implement data synchronization and cloud storage\n",
    "            5. Conduct thorough testing on multiple devices\n",
    "            6. Deploy to app stores and gather user feedback\n",
    "            Best regards,\n",
    "            AI Team\n",
    "            \"\"\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"requirements\": \"Create an e-commerce website with payment integration\"},\n",
    "        \"output\": {\n",
    "            \"email\": \"\"\"\n",
    "            Dear Client,\n",
    "            Based on your requirements: Create an e-commerce website with payment integration\n",
    "            We propose the following tasks:\n",
    "            1. Analyze business requirements and select appropriate e-commerce platform\n",
    "            2. Design responsive and user-friendly website layout\n",
    "            3. Set up product catalog and inventory management system\n",
    "            4. Implement secure payment gateway integration\n",
    "            5. Develop order processing and shipment tracking features\n",
    "            6. Conduct thorough security and performance testing\n",
    "            7. Launch website and provide post-launch support\n",
    "            Best regards,\n",
    "            AI Team\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = GraphAgentOptimizer(original_code, ground_truth, llm)\n",
    "optimizer.load_graph()\n",
    "optimizer.optimize(num_iterations=3)\n",
    "optimized_code = optimizer.get_optimized_code()\n",
    "\n",
    "print(\"Optimized code:\")\n",
    "print(optimized_code)\n",
    "\n",
    "# Test the optimized code\n",
    "exec(optimized_code, globals())\n",
    "test_input = {\"requirements\": \"Design a customer loyalty program for a retail chain\"}\n",
    "result = generate_email(test_input[\"requirements\"])\n",
    "print(\"Optimized graph result:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
